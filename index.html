<html>
<head> 
	<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171772141-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171772141-1');
  gtag('set', {'user_id': 'USER_ID'}); // Set the user ID using signed-in user_id.
  ga('set', 'userId', 'USER_ID'); // Set the user ID using signed-in user_id.
</script>

<title> Elizabeth Hall </title>
</head>
<style>
@font-face { font-family: Montserrat; src: url('Montserrat/Montserrat-Regular.ttf'); } 

h1 {text-align: center;}
h2 {padding-top: 35px;
	text-align:justify;
    margin-left: auto;
    margin-right: auto; width: 33em}
h3 {
  font-size: 15px;
}

p {text-align: center;}
div {text-align: center;}
img.center {
  padding-top: 75px;
  display: block;
  margin-left: auto;
  margin-right: auto;
  border-radius: 50%;
  width: 200px;
  height: 200px;
  
}
* {
    font-family: Montserrat;
  }

  P.blocktext {
  	text-align:justify;
    margin-left: auto;
    margin-right: auto;
    width: 50em;

}
 P.blocktext1 {
  	text-align:center;
    margin-left: auto;
    margin-right: auto;
    width: 50em;
    padding-top: 20px;
}

 P.blocktext2 {
  	text-align: justify;
    margin-left: auto;
    margin-right: auto;
    width: 60em;
    font-size: 14px;
    padding-bottom: 30px;

}
 hr.striped-border { border: 1px dashed #000; width: 50%; margin: auto; margin-bottom: 2%; }

.tab {
  padding-left: 30px;
}
img.left {
  float: left;
  padding-right: 40px;
  width: 200px;
  height: 140px;
}


</style>
<body>
	<img src="images/beth.png" alt="beth" class="center">

	<h1 style="font-size:3vw">Elizabeth Hall</h1>
	<P class="blocktext"> I am a graduate student in Psychology at UC Davis.  I work with Joy Geng in the <a style="text-decoration: none;" href="http://genglab.ucdavis.edu/"> Integrated Attention Lab</a> studying visual perception and memory in the human brain. I previously worked with Chris Baker in the <a style="text-decoration: none;" href="https://www.nimh.nih.gov/research/research-conducted-at-nimh/research-areas/clinics-and-labs/lbc/slp/index.shtml"> Laboratory of Brain and Cognition</a> at the NIH and with Doug Davidson at the <a style="text-decoration: none;" href="https://www.bcbl.eu/en"> Basque Center for Cognition, Brain, and Language.</a> </P>

	<P class="blocktext1"> ehhall @ ucdavis dot edu <span class="tab"> <a style="text-decoration: none;" href="https://scholar.google.com/citations?user=YYpSEMUAAAAJ&hl=en">google scholar</a>  <span class="tab"> <a style="text-decoration: none;" href="https://twitter.com/vision_beth">twitter</a> <span class="tab">  <a style="text-decoration: none;" href="https://github.com/ehhall">github</a> </P>
<h2> news </h2>
<hr class="striped-border">
<P class="blocktext"> 9/2021: Two new preprints added! Also completed the <a style="text-decoration: none;" href="https://github.com/NeuromatchAcademy/course-content-dl">Deep Learning</a> section of Neuromatch!</a></P>
<P class="blocktext"> 9/2020: Work with my old mentors Chris Baker and Wilma Bainbridge looking at encoding and recall of object / scenes in 7T fMRI is now out in <a style="text-decoration: none;" href="https://academic.oup.com/cercor/advance-article/doi/10.1093/cercor/bhaa329/6025502"> Cerebral Cortex! </a></P>
<P class="blocktext"> 7/2020: I participated in  <a style="text-decoration: none;" href="https://github.com/NeuromatchAcademy/course-content">Neuromatch Academy</a>, and got this <a style="text-decoration: none;" href="images/nma-certificate.pdf">fancy certificate!</a> </P>
<P class="blocktext"> 6/2020: Updated this website! </P>
<P class="blocktext"> 5/2020: I won <I> Most Creative Methodology </I> and tied for first place for <I> Best Grad Talk </I> at the UC Davis Spring Psychology Conference.  </P>
<P class="blocktext"> 4/2020: I was awarded the National Defense Science and Engineering Graduate Fellowship to pursue work on <a style="text-decoration: none;" href="https://youtu.be/IfWYcjTKRFA">visual attention in virtual reality.</a> </P>

<h2> research </h2>
<hr class="striped-border">
<P class="blocktext2"> <a href="https://psyarxiv.com/2az8x"> <img src="images/multicat.jpg" alt="multicat" class="left"> </a><B> <a style="text-decoration: none;" href="https://psyarxiv.com/2az8x">Highly similar and competing visual scenes lead to diminished object but not spatial detail in memory drawings </B> </a> <br>
	<I> Elizabeth H. Hall, Wilma A. Bainbridge Chris I. Baker </I> <br>
	Preprint. <br>
	  We investigated the detail and errors participants can have in memory when having to recall multiple, similar scenes. We found that memory drawings of "competing" scenes have diminished object detail, but are surpisingly still fairly spatially accurate.</P>

<P class="blocktext2"> <a href="https://psyarxiv.com/fqtvx"> <img src="images/candace.jpg" alt="candace" class="left"> </a><B> <a style="text-decoration: none;" href="https://psyarxiv.com/fqtvx">Objects are Prioritized for Attention Based Upon Meaning During Active Scene Viewing </B> </a> <br>
	<I> Candace Peacock, Elizabeth H. Hall, John M. Henderson</I> <br>
	Preprint. <br>
	 We looked at whether fixations were more likely to land on high-meaning objects in scenes. We found that fixations are more likely to be directed to high meaning objects than low meaning objects regardless of object salience.</P>

<P class="blocktext2"> <a href="https://academic.oup.com/cercor/advance-article/doi/10.1093/cercor/bhaa329/6025502"> <img src="images/hipporecall.jpg" alt="hipporecall" class="left"> </a><B> <a style="text-decoration: none;" href="https://academic.oup.com/cercor/advance-article/doi/10.1093/cercor/bhaa329/6025502">Distinct representational structure and localization for visual encoding and recall during visual imagery </B> </a> <br>
	<I> Wilma A. Bainbridge, Elizabeth H. Hall, Chris I. Baker </I> <br>
	Cerebral Cortex, 08 December 2020 <br>
	 We find representations of memory content during recall show key differences from encoding in granularity of detail & spatial distribution. We found that peak encoding & recall similarity is anterior to encoding peaks.  </P>

<P class="blocktext2"> <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02915/full"> <img src="images/frontiers.jpg" alt="frontiers" class="left"> </a><B> <a style="text-decoration: none;" href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02915/full">Eye Movements in Real-World Scene Photographs: General Characteristics and Effects of Viewing Task</B> </a> <br>
	<I> Deborah A. Cronin, Elizabeth H. Hall, Jessica E. Goold, Taylor R. Hayes and John M. Henderson </I> <br>
	Frontiers in Psychology, 14 January 2020. <br>
	We examined effects of viewing task on when and where the eyes move in real-world scenes during memorization and an aesthetic judgment tasks. Temporal- and distribution-level analyses reveal significant task-driven differences in eye movement behavior. </P>
	
<P class="blocktext2"> <a href="https://www.nature.com/articles/s41467-018-07830-6"> <img src="images/memrecall.png" alt="memrecall" class="left"> </a><B> <a style="text-decoration: none;" href="https://www.nature.com/articles/s41467-018-07830-6">Drawings of real-world scenes during free recall reveal detailed object and spatial information in memory </B> </a> <br>
	<I> Wilma A. Bainbridge, Elizabeth H. Hall, Chris I. Baker </I> <br>
	Nature Communications, 02 January 2019. <br>
	Participants studied 30 scenes and drew as many images in as much detail as possible from memory. The resulting memory-based drawings were scored by thousands of online observers, revealing numerous objects, few memory intrusions, and precise spatial information. </P>
</body>
</html>


